---
title: "BERT"
description: "BERT stands for Bidirectional Encoder Representations from Transformers. You can fine-tune it and get state-of-the-art results in a wide variety of natural language processing tasks"
---

<TextArea>
## About BERT

The BERT paper by Jacob Devlin was released not long after the publication of the first GPT model. It achieved significant improvements on many important NLP benchmarks, such as GLUE. Since then, their ideas have influenced many state-of-the-art models in language understanding. Bidirectional Encoder Representations from Transformers (BERT) is a natural language processing technique (NLP) that was proposed in 2018. (NLP is the field of artificial intelligence aiming for computers to read, analyze, interpret and derive meaning from text and spoken words. This practice combines linguistics, statistics, and Machine Learning to assist computers in ‘understanding’ human language.) BERT is based on the idea of pretraining a transformer model on a large corpus of text and then fine-tuning it for specific NLP tasks. The transformer model is a deep learning model that is designed to handle sequential data, such as text. The bidirectional transformer architecture stacks encoders from the original transformer on top of each other. This allows the model to better capture the context of the text. 

</TextArea>
<Libraries>
  <Library
    name="BERT Model"
    language="Python"
    description="Get the basic BERT pre-trained model from TensorFlowHub and fine tune to your needs"
    href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4"
  />
  <Library
    name="Text Classification with BERT"
    language="Python"
    description="How to leverage a pre-trained BERT model from Hugging Face to classify text of news articles"
    href="https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f"
  />
  <Library
    name="Question Answering with a fine-tuned BERT"
    language="Python"
    description="using Hugging Face Transformers and PyTorch on CoQA dataset by Stanford"
    href="https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626"
  />
</Libraries>
